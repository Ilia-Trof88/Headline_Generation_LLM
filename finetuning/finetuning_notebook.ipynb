{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/project_news_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—è–¥–æ–≤ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: 4373\n",
      "      –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—è–¥–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: 486\n"
     ]
    }
   ],
   "source": [
    "df_path = '{your_dataset_path}'\n",
    "\n",
    "main_df = pd.read_csv(df_path)\n",
    "\n",
    "main_df.drop(columns = ['Unnamed: 0', '–¢—ç–≥–∏', '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏'], inplace = True)\n",
    "#–£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—É–¥—É—Ç –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã –≤ –æ–±—É—á–µ–Ω–∏–∏\n",
    "\n",
    "model_instructions = \"\"\"\n",
    "    –¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.\n",
    "    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–π –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç—Ä–∞–∂–∞–ª –±—ã —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n",
    "    –ù–µ –ø–æ—è—Å–Ω—è–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç. –¢–≤–æ–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–∏—á–µ–≥–æ –±–æ–ª–µ–µ.\n",
    "    \"\"\"\n",
    "main_df['–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏'] = model_instructions\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(main_df, test_size = 0.1, random_state = 1337)\n",
    "\n",
    "train_dataset = train_dataset.reset_index().drop(columns = ['index'])\n",
    "test_dataset = test_dataset.reset_index().drop(columns = ['index'])\n",
    "\n",
    "\n",
    "print(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—è–¥–æ–≤ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(train_dataset)}\\n\\\n",
    "      –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—è–¥–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1048\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/gemma-2-2b',\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, #–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 16, –æ–¥–Ω–∞–∫–æ –ø–æ—Å–∫–æ–ª—å–∫—É –º–æ–¥–µ–ª—å –º–∞–ª–µ–Ω—å–∫–∞—è\n",
    "    lora_alpha = 16, #Scaling factor –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è LoRa - –æ–±—ã—á–Ω–æ —Ä–∞–≤–µ–Ω R –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤.\n",
    "    lora_dropout = 0, \n",
    "    bias = 'none',\n",
    "    use_gradient_checkpointing = 'unsloth',\n",
    "    random_state = 1337,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:\n",
    "1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.\n",
    "2. –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "3. –û—Ç–≤–µ—Ç - —Ç–≤–æ–π –æ—Ç–≤–µ—Ç.\n",
    "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.  \n",
    "### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å:\n",
    "{}\n",
    "\n",
    "### –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
    "{}\n",
    "\n",
    "### –û—Ç–≤–µ—Ç:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token #EOS –∏–ª–∏ —Ç–æ–∫–µ–Ω –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(examples):\n",
    "    instructions = examples['–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏']\n",
    "    input = examples['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏']\n",
    "    output = examples['–ó–∞–≥–æ–ª–æ–≤–æ–∫']\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for instructions, input, output in zip(instructions, input, output):\n",
    "        text = alpaca_prompt.format(instructions, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {'text': texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4373 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4373/4373 [00:00<00:00, 34735.49 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 486/486 [00:00<00:00, 42752.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "\n",
    "train_dataset = train_dataset.map(format_prompt, batched = True)\n",
    "test_dataset = test_dataset.map(format_prompt, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4373/4373 [00:03<00:00, 1318.71 examples/s]\n",
      "Map (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 486/486 [00:01<00:00, 311.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    dataset_text_field = 'text',\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)], #–î–æ–±–∞–≤–ª—è–µ–º ES == 5 —ç–ø–æ—Ö–∞–º.\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 4000,\n",
    "        eval_strategy = 'steps',\n",
    "        metric_for_best_model = 'eval_loss',\n",
    "        load_best_model_at_end = True,\n",
    "        greater_is_better = False,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1337,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{insert_your_gemma2_2b_save_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 4,373 | Num Epochs = 8\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 4,000\n",
      " \"-____-\"     Number of trainable parameters = 20,766,720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/4000 44:48 < 2:14:41, 0.37 it/s, Epoch 1/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.450200</td>\n",
       "      <td>2.142085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.372700</td>\n",
       "      <td>2.085290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.368100</td>\n",
       "      <td>2.041223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.320200</td>\n",
       "      <td>2.026646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>2.009410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.265800</td>\n",
       "      <td>2.035812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.203600</td>\n",
       "      <td>2.015994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>2.031903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.170300</td>\n",
       "      <td>2.026358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.202000</td>\n",
       "      <td>2.017031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('gemma2_2b_finetuned/tokenizer_config.json',\n",
       " 'gemma2_2b_finetuned/special_tokens_map.json',\n",
       " 'gemma2_2b_finetuned/tokenizer.model',\n",
       " 'gemma2_2b_finetuned/added_tokens.json',\n",
       " 'gemma2_2b_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9216, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(train_dataset['–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏'][0],\n",
    "                             test_dataset['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏'][3],\n",
    "                             \"\") #Response left blank for generation\n",
    "    ], return_tensors = 'pt').to('cuda')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True, pad_token_id = tokenizer.eos_token_id)\n",
    "result = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥–æ–ª–æ–≤–æ–∫, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é: \n",
      "–¢–æ–º—Å–∫—Å—Ç–∞—Ç –Ω–∞–∑–≤–∞–ª –ø—Ä–æ–¥—É–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–æ—Ä–æ–∂–∞–ª–∏ –≤ –∏—é–Ω–µ, –∞ —Ç–∞–∫–∂–µ —É—Å–ª—É–≥–∏<eos>\n",
      "–û–±—Ä–∞–∑—Ü–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫:\n",
      "–ö–∞–∫ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å —Ü–µ–Ω—ã –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ —É—Å–ª—É–≥–∏ –≤ –¢–æ–º—Å–∫–µ –∑–∞ –º–µ—Å—è—Ü: –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n"
     ]
    }
   ],
   "source": [
    "print(f\"–ó–∞–≥–æ–ª–æ–≤–æ–∫, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é: {result[0].split('–û—Ç–≤–µ—Ç:', 1)[1]}\\n–û–±—Ä–∞–∑—Ü–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫:\\n{test_dataset['–ó–∞–≥–æ–ª–æ–≤–æ–∫'][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤–ø–æ–ª–Ω–µ –ø—Ä–∏–µ–º–ª–∏–º—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: str, train_set: pd.DataFrame, eval_set: pd.DataFrame) -> None:\n",
    "    '''–§—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Unsloth.\n",
    "    –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ - –¥–æ–æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "    \n",
    "    Args:\n",
    "        model (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π Unsloth.\n",
    "        train_set (pd.DataFrame): –î–∞—Ç–∞—Ñ—Ä–µ–π–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤ —Å–µ–±–µ —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏:\\\n",
    "            1. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏;\n",
    "            2. –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏;\n",
    "            3. –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "        test_set (pd.DataFrame): –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π train_set\n",
    "    Returns:\n",
    "        None - –§—É–Ω–∫—Ü–∏—è –Ω–µ –≤–æ–∑—Ä–∞—â–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è:\\\n",
    "        –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥—É—Ç –∑–∞–ø–∏—Å–∞–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é,\\\n",
    "        –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—é –≤—ã–±—Ä–∞–Ω–Ω–æ–π –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "    '''\n",
    "    \n",
    "    model_name = model\n",
    "\n",
    "    \n",
    "    print(f'–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_name}...')\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model,\n",
    "        max_seq_length = 1048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True\n",
    "    )\n",
    "    \n",
    "    def clear_cache(model, tokenizer):\n",
    "        '''–§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ GPU'''\n",
    "        with torch.no_grad():\n",
    "            model.cpu()\n",
    "        del model\n",
    "        del tokenizer\n",
    "    \n",
    "        gc.collect()\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    alpaca_prompt = \"\"\"–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:\n",
    "    1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.\n",
    "    2. –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "    3. –û—Ç–≤–µ—Ç - —Ç–≤–æ–π –æ—Ç–≤–µ—Ç.\n",
    "    –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.  \n",
    "    ### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å:\n",
    "    {}\n",
    "\n",
    "    ### –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
    "    {}\n",
    "\n",
    "    ### –û—Ç–≤–µ—Ç:\n",
    "    {}\"\"\"\n",
    "\n",
    "    def format_prompt(examples):\n",
    "        '''–§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç alpaca_prompt_template –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–¥–∞—á–µ–π.\n",
    "        \n",
    "        Args:\n",
    "            examples (pandas.dataframe): –î–∞—Ç–∞—Ñ—Ä–µ–π–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–ª–æ–Ω–∫–∏:\n",
    "                                        1. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏.\n",
    "                                        2. –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏.\n",
    "                                        3. –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "        \n",
    "        Returns:\n",
    "            dict - –°–ª–æ–≤–∞—Ä—å: –∫–ª—é—á - —Å—Ç—Ä–æ–∫–∞ 'text', –∑–Ω–∞—á–µ–Ω–∏–µ - –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π prompt_template\n",
    "        '''\n",
    "        instructions = examples['–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏']\n",
    "        input = examples['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏']\n",
    "        output = examples['–ó–∞–≥–æ–ª–æ–≤–æ–∫']\n",
    "\n",
    "        texts = []\n",
    "\n",
    "        for instructions, input, output in zip(instructions, input, output):\n",
    "            text = alpaca_prompt.format(instructions, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        \n",
    "        return {'text': texts}\n",
    "    \n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train_set)\n",
    "    test_dataset = Dataset.from_pandas(eval_set)\n",
    "\n",
    "    train_dataset = train_dataset.map(format_prompt, batched = True)\n",
    "    test_dataset = test_dataset.map(format_prompt, batched = True)\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = 'none',\n",
    "    use_gradient_checkpointing = 'unsloth',\n",
    "    random_state = 1337,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "    trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    dataset_text_field = 'text',\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)], #–î–æ–±–∞–≤–ª—è–µ–º ES == 5 —ç–ø–æ—Ö–∞–º.\n",
    "    max_seq_length = 1048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 5000,\n",
    "        eval_strategy = 'steps',\n",
    "        metric_for_best_model = 'eval_loss',\n",
    "        load_best_model_at_end = True,\n",
    "        greater_is_better = False,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1337,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    ")\n",
    "    save_path = model_name\n",
    "\n",
    "    print(f'–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}...')\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    clear_cache(model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø–∏—à–µ–º —Ü–∏–∫–ª for –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = ['unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit',\n",
    "                   'unsloth/gemma-2-9b-bnb-4bit']\n",
    "\n",
    "for model in selected_models:\n",
    "    \n",
    "    train_model(train_set = train_dataset,\n",
    "                eval_set = test_dataset,\n",
    "                model = model)\n",
    "    \n",
    "    print(f\"–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model} –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
