# Генерация новостных заголовков с использованием современных языковых моделей

## О проекте

Данный проект представляет собой современную реализацию и развитие идей, представленных в магистерской диссертации:

> Трофимов И. П. Дообучение нейросети ruGPT-3 для генерации новостных заголовков: формальная оценка влияния новостной рубрики на качество заголовка: магистерская диссертация по направлению подготовки: 45.04.03 - Фундаментальная и прикладная лингвистика / Трофимов И. П., Рахимова З. Я. - Томск, 2024. URL: https://vital.lib.tsu.ru/vital/access/manager/Repository/vital:22017 

Оригинальное исследование фокусировалось на использовании модели ruGPT-3 для генерации новостных заголовков для новостных текстов различных тематик, в то время как данный проект адаптирует методологию с применением современных больших языковых моделей (LLM) с открытым исходным кодом. Это позволяет не только проверить актуальность первоначальных выводов, но и исследовать новые возможности в задаче генерации новостных заголовков.

## Основные отличия от оригинального исследования

- Использование современных больших языковых моделей (LLM) с открым исходным кодом:
  - Gemma-2-2B (4-bit квантование)
  - Gemma-2-9B (4-bit квантование)
  - Mistral-Nemo (4-bit квантование)
  Все модели используется в квантованной версии (4 бита) для оптимизации использования вычислительных ресурсов в условиях ограниченных вычислительных мощностей. 
- Добавление метрики оценивания семантической близости (BertScore).
- Удаление метрики METEOR.


### Результаты оригинального исследования (RuGPT-3)

| Модель | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L | METEOR |
|:-------|:-----:|:--------:|:--------:|:--------:|:-------:|
|RuGPT-3 (все тематики) | 0.045 | 0.25 | 0.11 | 0.22 | 0.2 |
|RuGPT-3 (Здравоохранение) | 0.052 | 0.15 | 0.06 | 0.14 | 0.25 |
|RuGPT-3 (Томские новости) | 0.022 | 0.08 | 0.02 | 0.08 | 0.19 |
|RuGPT-3 (Происшествия) | 0.037 | 0.06 | 0.007 | 0.06 | 0.23 |
|RuGPT-3 (Экономика и политика) | 0.037 | 0.09 | 0.01 | 0.09 | 0.18 |


## Результаты настоящего исследования

### Zero-shot генерация

| Модель | BertScore |  |  | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L |
|:-------|:---------:|:--------:|:--------:|:-----:|:--------:|:--------:|:--------:|
| | P | R | F1 | | | | |
|Gemma-2-2b (4-bit) | 0.70 | 0.68 | 0.64 | 0.00029 | 0.10 | 0.03 | 0.08 |
|Gemma-2-9b (4-bit) | 0.70 | 0.75 | 0.72 | 0.00038 | 0.22 | 0.11 | 0.20 |
|Mistral-Nemo (4-bit) | 0.72 | 0.76 | 0.74 | 0.00074 | 0.24 | 0.11 | 0.22 |

### После дообучения (Fine-tuning)

| Модель | BertScore |  |  | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L |
|:-------|:---------:|:--------:|:--------:|:-----:|:--------:|:--------:|:--------:|
| | P | R | F1 | | | | |
|Gemma-2-2b (4-bit) | 0.77 | 0.77 | 0.77 | 0.00079 | 0.31 | 0.16 | 0.29 |
|Gemma-2-9b (4-bit) | 0.79 | 0.79 | 0.79 | 0.0008 | 0.36 | 0.21 | 0.34 |
|Mistral-Nemo (4-bit) | 0.79 | 0.79 | 0.79 | 0.00082 | 0.34 | 0.19 | 0.32 |

где:
- P - Precision (точность)
- R - Recall (полнота)
- F1 - F1-мера

## Сравнение оригинального исследования с настоящим

Основные отличия нового исследования от оригинального заключаются в использовании современных открытых языковых моделей, которые демонстрируют существенный прогресс в качестве генерации заголовков. В то время как наше оригинальное исследование опиралось на единственную модель RuGPT-3-Small, новый подход включает сравнение трех различных моделей: Gemma-2-2b, Gemma-2-9b и Mistral-Nemo, что позволило провести более комплексный анализ. Важным методологическим улучшением стало добавление метрики BertScore для оценки семантической близости генерируемых заголовков.

Результаты показывают, что современные модели даже без дополнительного обучения (в режиме Zero-shot) достигают показателей, сопоставимых с дообученной RuGPT-3, а после тонкой настройки значительно превосходят её. Особенно важным наблюдением стало то, что даже относительно небольшая модель Gemma-2-2b (в 4.5 раза меньше Gemma-2-9b и в 6 раз меньше Mistral-Nemo) после дообучения показывает результаты, близкие к более крупным моделям, что открывает новые возможности для практического применения. В отличие от предыдущего исследования, где качество генерации сильно зависело от тематики новостей, новые модели демонстрируют более стабильные результаты, что говорит об их лучшей универсальности.

Использование открытых моделей и улучшенные результаты на меньших моделях делают новое исследование более воспроизводимым и практически применимым, что является существенным шагом вперед по сравнению с оригинальной работой. Количественные показатели также демонстрируют значительный прогресс: если лучшие результаты RuGPT-3 по метрике ROUGE-1 составляли 0.25, то новые модели после дообучения достигают значений 0.31-0.36, что говорит об улучшении качества генерации на 40-50%.

## Структура проекта

Проект состоит из следующих директорий:
1. data_preprocessing.
  - Директория содержит в себе ноутбуке и скрипт для преобработки текстовых данных.
2. finetuning.
  - Директория содержит ноутбук, посвященный дообучения языковых моделей с использование библиотеки unsloth.
3. generation_evaluation.
  - Директория содержит ноутбук оценки результатов генерации (сгенерированных заголовков) больших языковых моделей.
    Была проведена оценка как Zero-Shot генерации, так и генерации дообученных моделей.
4. model_inference.
  - Директория содержит ноутбуки инференса (запуска) моделей.
5. parsing.
  - Директория содержит скрипт парсинга новостей с сайта новостного агентства Vtomske.ru. URL: https://vtomske.ru/.

## Как использовать

Требования для запуска проекта:<br>
1. Установленные Anaconda или Miniconda
2. Git для клонирования репозитория

## Создание conda-окружения

1. Клонируйте данный репозиторий
```bash
git clone [https://github.com/Ilia-Trof88/Headline_Generation_LLM]
cd [название директории проекта]
```

2. Создайте conda-окружение из файла environment.yml:
```bash
conda env create -f environment.yml
```

3. Активируйте созданное окружение:
```bash
# Для Windows
conda activate [название_окружения]

# Для Linux/MacOS
source activate [название_окружения]
```
Однако из-за возможного конфликта версий CUDA и библиотеки torch, настоятельно рекомендуется создать новую среду, следуя инструкциям из официального репозитория библиотеки unsloth: https://github.com/unslothai/unsloth/tree/main#-installation-instructions

## Авторы оригинальной диссертации

1. Трофимов Илья Павлович. (Санкт-Петербургский государственный электротехнический университет «ЛЭТИ» им. В.И. Ульянова (Ленина), Национальный исследовательский Томский государственный университет)
2. Рахимова Зульфия Якубовна. (Национальный исследовательский Томский государственный университет)

## Автор данного проекта

1. Трофимов Илья Павлович.