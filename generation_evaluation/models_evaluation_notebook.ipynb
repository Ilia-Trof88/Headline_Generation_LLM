{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Performance of Zero-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/work5/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/opt/miniconda3/envs/project_news_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /home/work5/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from evaluate import load\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Заголовок</th>\n",
       "      <th>Текст новости</th>\n",
       "      <th>Дата публикации</th>\n",
       "      <th>Тэги</th>\n",
       "      <th>prompt</th>\n",
       "      <th>unsloth/gemma-2-2b_headline</th>\n",
       "      <th>unsloth/gemma-2-9b-bnb-4bit_headline</th>\n",
       "      <th>unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Банки предложили блокировать карты россиян при...</td>\n",
       "      <td>Банки выступили с инициативой блокировать карт...</td>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>'Томск', 'Экономика', 'Банки'</td>\n",
       "      <td>Ты - профессиональный журналист с многолетним ...</td>\n",
       "      <td>Ответ:\\nБанки выступили с инициативой блокиров...</td>\n",
       "      <td>Ответ:\\nБанки выступили с инициативой блокиров...</td>\n",
       "      <td>Ответ:\\nБанки предложили блокировать карты кли...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Заголовок  \\\n",
       "0  Банки предложили блокировать карты россиян при...   \n",
       "\n",
       "                                       Текст новости Дата публикации  \\\n",
       "0  Банки выступили с инициативой блокировать карт...      2019-08-20   \n",
       "\n",
       "                            Тэги  \\\n",
       "0  'Томск', 'Экономика', 'Банки'   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Ты - профессиональный журналист с многолетним ...   \n",
       "\n",
       "                         unsloth/gemma-2-2b_headline  \\\n",
       "0  Ответ:\\nБанки выступили с инициативой блокиров...   \n",
       "\n",
       "                unsloth/gemma-2-9b-bnb-4bit_headline  \\\n",
       "0  Ответ:\\nБанки выступили с инициативой блокиров...   \n",
       "\n",
       "  unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit_headline  \n",
       "0  Ответ:\\nБанки предложили блокировать карты кли...    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = '{your_dataset_path}'\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_score(refs: list, preds: list)->dict:\n",
    "    '''Функция расчитывает средний показатель метрики BertScore.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        refs (list): Список образцов-эталонов.\n",
    "        preds (list): Список образцов-кандидатов.\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        dict - словарь, содержащий в себе следующие ключи:\n",
    "            Mean Precision - Средний показатель метрики BERT-P\n",
    "            Mean Recall - Средний показатель метрики BERT-R\n",
    "            Mean F1 - Средне-гармоническое меджду BERT-P и BERT-R\n",
    "\n",
    "    '''\n",
    "    bert_score = load('bertscore')\n",
    "\n",
    "    predictions = preds\n",
    "\n",
    "    references = refs\n",
    "\n",
    "    result = bert_score.compute(predictions = predictions, references = references, lang = 'ru')\n",
    "\n",
    "    mean_precision = sum(result['precision']) / len(result['precision'])\n",
    "\n",
    "    mean_recall = sum(result['recall']) / len(result['recall'])\n",
    "\n",
    "    mean_f1 = sum(result['f1']) / len(result['f1'])\n",
    "\n",
    "    united_dictionary = {\n",
    "        'Mean Precision': round(mean_precision, 2),\n",
    "        'Mean Recall': round(mean_recall, 2),\n",
    "        'Mean F1': round(mean_f1, 2)\n",
    "    }\n",
    "\n",
    "    return united_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка результатов, сгенерированных моделями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатели метрики BertScore для модели unsloth/gemma-2-2b: \n",
      "{'Mean Precision': 0.61, 'Mean Recall': 0.68, 'Mean F1': 0.64}\n",
      "Показатели метрики BertScore для модели unsloth/gemma-2-9b-bnb-4bit: \n",
      "{'Mean Precision': 0.7, 'Mean Recall': 0.75, 'Mean F1': 0.72}\n",
      "Показатели метрики BertScore для модели unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit: \n",
      "{'Mean Precision': 0.72, 'Mean Recall': 0.76, 'Mean F1': 0.74}\n"
     ]
    }
   ],
   "source": [
    "col_to_evaluate = list(df.columns[5:]) #Переменная\n",
    "\n",
    "model_name_extractions_pattern = r\".*[^_headline]\"\n",
    "\n",
    "reference_col = df['Заголовок'].tolist()\n",
    "\n",
    "for col in col_to_evaluate:\n",
    "    model_name = re.findall(model_name_extractions_pattern, col)[0]\n",
    "    \n",
    "    print(f\"Показатели метрики BertScore для модели {model_name}: \")\n",
    "\n",
    "    results = bert_score(refs = reference_col, preds = df[col].tolist())\n",
    "\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, модели Gemma2:9b и Mistral_Nemo демонстрируют приемлемые результаты семантического, даже при Zero-Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/work5/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(refs:str, preds:str)->float:\n",
    "    '''Функция выполняет расчет показателя метрики BLEU по двум объектам.\n",
    "    Args:\n",
    "        refs (str): Образцы-эталоны\n",
    "        preds (str): Образцы-кандидаты (предсказания модели)\n",
    "    Returns:\n",
    "        float - числовой показатель метрики BLEU\n",
    "    '''\n",
    "    smooth = SmoothingFunction() #Важно добавить SmoothingFunction, поскольку если нет пересекающихся n-грамм, BLEU будет равен 0\n",
    "\n",
    "    refs_tokenized, preds_tokenized = word_tokenize(refs), word_tokenize(preds)\n",
    "\n",
    "    weights = (0.25, 0.25, 0.25, 0.25)\n",
    "\n",
    "    return sentence_bleu(references = refs_tokenized, hypothesis = preds_tokenized,\n",
    "                         weights = weights, smoothing_function=smooth.method4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция расчитывающая Bleu_score на всем корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_corpus(references: list, predictions: list) -> float:\n",
    "    '''Функция выполняет расчет метрики BLEU по всему корпусу.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        references (list): Список, содержащий в себе образцы-эталоны.\n",
    "        predictions (list): Список, содержащий в себе образцы-кандидаты, т.е. сгенерированные моделью образцы\n",
    "    \n",
    "    Returns:\n",
    "        float - числовой показатель метрики BLEU\n",
    "    '''\n",
    "\n",
    "    for index in range(0, len(references)):\n",
    "        references[index] = word_tokenize(references[index], language = 'russian')\n",
    "    \n",
    "    for index in range(0, len(predictions)):\n",
    "        predictions[index] = word_tokenize(predictions[index], language = 'russian')\n",
    "    \n",
    "    weights = (0.25, 0.25, 0.25, 0.25)\n",
    "\n",
    "    smooth = SmoothingFunction()\n",
    "\n",
    "    return round(corpus_bleu(\n",
    "        list_of_references = references,\n",
    "        hypotheses = predictions,\n",
    "        weights = weights,\n",
    "        smoothing_function = smooth.method4\n",
    "    ), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/gemma-2-2b\n",
      "Показатели метрики BertScore для модели unsloth/gemma-2-2b: \n",
      "0.00029\n",
      "unsloth/gemma-2-9b-bnb-4bit\n",
      "Показатели метрики BertScore для модели unsloth/gemma-2-9b-bnb-4bit: \n",
      "0.00038\n",
      "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\n",
      "Показатели метрики BertScore для модели unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit: \n",
      "0.00074\n"
     ]
    }
   ],
   "source": [
    "#Расчет BLEU-Score\n",
    "for col in col_to_evaluate:\n",
    "    model_name = re.findall(model_name_extractions_pattern, col)[0]\n",
    "\n",
    "    reference_col = df['Заголовок'].tolist()\n",
    "\n",
    "    print(model_name)\n",
    "    \n",
    "    print(f\"Показатели метрики BertScore для модели {model_name}: \")\n",
    "\n",
    "    results = calculate_bleu_corpus(references = reference_col, predictions = df[col].tolist())\n",
    "\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE-Score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_score(prediction: str, reference: str)-> str:\n",
    "    '''Функция расчитывает показатели метрики Rouge-1, Rouge-2 и Rouge-L\n",
    "    Args:\n",
    "        predictions (str): Строка, содержщая предсказание модели\n",
    "        reference (str): Строка, содержащая эталонную строку\n",
    "    Returns:\n",
    "        dict - словарь, содержащий в себе показатели F-меры метрик, указанных выше \n",
    "    '''\n",
    "    \n",
    "    def tokenize_russian(text):\n",
    "        '''Мы должны добавить дополнительную функцию токенизации на русском языке,\n",
    "        посколько функции расчета метрики Rouge из библиотеки rouge_score могут обрабатывать русские предложения некорректно'''\n",
    "        return ' '.join(word_tokenize(text, language = 'russian'))\n",
    "    \n",
    "    prediction = tokenize_russian(prediction)\n",
    "    reference = tokenize_russian(reference)\n",
    "\n",
    "    class SimpleTokenizer:\n",
    "        def tokenize(self, text):\n",
    "            return text.split()\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'],\n",
    "                                      tokenizer = SimpleTokenizer())\n",
    "    \n",
    "    results = scorer.score(prediction, reference)\n",
    "\n",
    "    final_dict = {\n",
    "        'Rouge1 F-measure': results['rouge1'].fmeasure,\n",
    "        'Rouge2 F-measure': results['rouge2'].fmeasure,\n",
    "        'RougeL F-measure': results['rougeL'].fmeasure,\n",
    "    }\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_to_evaluate:\n",
    "\n",
    "    df[f'rouge_score_of_{col}'] = '-'\n",
    "\n",
    "reference_col = 'Заголовок'\n",
    "\n",
    "for col in col_to_evaluate:\n",
    "    \n",
    "    current_column = col #Содержит текущую колонку\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #current_index = index #Содержит индекс текущей итерации\n",
    "\n",
    "        current_headline = df.at[index, reference_col] #Содержит образцовый заголовок\n",
    "\n",
    "        current_prediction = df.at[index, current_column] #Содержит предсказание\n",
    "\n",
    "        df.at[index, f'rouge_score_of_{col}'] = calculate_rouge_score(prediction = current_prediction,\n",
    "                                                            reference = current_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_rouge_score(df_column: str)-> dict:\n",
    "    '''Функция расчитывает средние показатели метрик Rouge-1, Rouge-2, Rouge-L.\n",
    "    \n",
    "    Args:\n",
    "        df_column (str): Название колонки в датафрейме, содержащая показатели приведенных выше метрик в виде словаря.\n",
    "\n",
    "    Returns:\n",
    "        dict - словарь, содержащий в себе средние показатели приведенных выше метрик.\n",
    "    '''\n",
    "\n",
    "    rouge_1 = []\n",
    "\n",
    "    rouge_2 = []\n",
    "\n",
    "    rouge_L = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        rouge_1.append(row[df_column]['Rouge1 F-measure'])\n",
    "        rouge_2.append(row[df_column]['Rouge2 F-measure'])\n",
    "        rouge_L.append(row[df_column]['RougeL F-measure'])\n",
    "    \n",
    "    final_dict = {\n",
    "        'Mean Rouge1 Score': round(sum(rouge_1) / len(rouge_1), 2),\n",
    "        'Mean Rouge2 Score': round(sum(rouge_2) / len(rouge_2), 2),\n",
    "        'Mean RougeL Score': round(sum(rouge_L) / len(rouge_L), 2),\n",
    "    }\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатели метрики Rouge для модели unsloth/gemma-2-2b Следующие: \n",
      "{'Mean Rouge1 Score': 0.1, 'Mean Rouge2 Score': 0.03, 'Mean RougeL Score': 0.08}\n",
      "Показатели метрики Rouge для модели unsloth/gemma-2-9b-bnb-4bit Следующие: \n",
      "{'Mean Rouge1 Score': 0.22, 'Mean Rouge2 Score': 0.11, 'Mean RougeL Score': 0.2}\n",
      "Показатели метрики Rouge для модели unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit Следующие: \n",
      "{'Mean Rouge1 Score': 0.24, 'Mean Rouge2 Score': 0.11, 'Mean RougeL Score': 0.22}\n"
     ]
    }
   ],
   "source": [
    "for col in col_to_evaluate:\n",
    "    print(f\"Показатели метрики Rouge для модели {re.findall(model_name_extractions_pattern, col)[0]} Следующие: \")\n",
    "\n",
    "    print(calculate_mean_rouge_score(f\"rouge_score_of_{col}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuned Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_df = pd.read_csv('{your_finetuned_dataset_path}')\n",
    "finetuned_df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "finetuned_df.rename(columns = {'unsloth/Mistral-Nemo_Finetuned_generated_headline':'unsloth/mistral-nemo_finetuned_generated_headline'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. BertScore Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для модели gemma2_2b показатели метрики BertScore следующие: \n",
      "{'Mean Precision': 0.77, 'Mean Recall': 0.77, 'Mean F1': 0.77}\n",
      "Для модели mistral-nemo показатели метрики BertScore следующие: \n",
      "{'Mean Precision': 0.79, 'Mean Recall': 0.79, 'Mean F1': 0.79}\n",
      "Для модели gemma-2_9b показатели метрики BertScore следующие: \n",
      "{'Mean Precision': 0.79, 'Mean Recall': 0.79, 'Mean F1': 0.79}\n"
     ]
    }
   ],
   "source": [
    "all_cols = list(finetuned_df.columns)\n",
    "\n",
    "needed_cols = []\n",
    "\n",
    "model_extraction_pattern = r\"unsloth/(.*?)_finetuned_generated_headline\"\n",
    "\n",
    "for col in all_cols:\n",
    "    if col.startswith('unsloth'):\n",
    "        needed_cols.append(col)\n",
    "\n",
    "reference_col = finetuned_df['Заголовок'].tolist()\n",
    "\n",
    "for column in needed_cols:\n",
    "    preds = finetuned_df[column].tolist()\n",
    "\n",
    "    print(f\"Для модели {re.findall(model_extraction_pattern, column)[0]} показатели метрики BertScore следующие: \")\n",
    "\n",
    "    print(bert_score(refs = reference_col, preds = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Calculating BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для модели gemma2_2b показатель BLEU-Score составил: \n",
      "0.00079\n",
      "Для модели mistral-nemo показатель BLEU-Score составил: \n",
      "0.00082\n",
      "Для модели gemma-2_9b показатель BLEU-Score составил: \n",
      "0.0008\n"
     ]
    }
   ],
   "source": [
    "for column in needed_cols:\n",
    "    reference_col = finetuned_df['Заголовок'].tolist()\n",
    "\n",
    "    print(f\"Для модели {re.findall(model_extraction_pattern, column)[0]} показатель BLEU-Score составил: \")\n",
    "\n",
    "    print(calculate_bleu_corpus(references = reference_col, predictions = finetuned_df[column].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Rouge-Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in needed_cols:\n",
    "\n",
    "    finetuned_df[f'rouge_score_of_{col}'] = '-'\n",
    "\n",
    "reference_col = 'Заголовок'\n",
    "\n",
    "for col in needed_cols:\n",
    "    \n",
    "    current_column = col #Содержит текущую колонку\n",
    "\n",
    "    for index, row in finetuned_df.iterrows():\n",
    "        \n",
    "        #current_index = index #Содержит индекс текущей итерации\n",
    "\n",
    "        current_headline = finetuned_df.at[index, reference_col] #Содержит образцовый заголовок\n",
    "\n",
    "        current_prediction = finetuned_df.at[index, current_column] #Содержит предсказание\n",
    "\n",
    "        finetuned_df.at[index, f'rouge_score_of_{col}'] = calculate_rouge_score(prediction = current_prediction,\n",
    "                                                            reference = current_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Немного изменим наш изначальный концепт функции расчета среднего Rouge\n",
    "def calculate_mean_rouge_score(df: pd.DataFrame, df_column: str)-> dict:\n",
    "    '''Функция расчитывает средние показатели метрик Rouge-1, Rouge-2, Rouge-L.\n",
    "    \n",
    "    Args:\n",
    "        df_column (str): Название колонки в датафрейме, содержащая показатели приведенных выше метрик в виде словаря.\n",
    "\n",
    "    Returns:\n",
    "        dict - словарь, содержащий в себе средние показатели приведенных выше метрик.\n",
    "    '''\n",
    "\n",
    "    rouge_1 = []\n",
    "\n",
    "    rouge_2 = []\n",
    "\n",
    "    rouge_L = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        rouge_1.append(row[df_column]['Rouge1 F-measure'])\n",
    "        rouge_2.append(row[df_column]['Rouge2 F-measure'])\n",
    "        rouge_L.append(row[df_column]['RougeL F-measure'])\n",
    "    \n",
    "    final_dict = {\n",
    "        'Mean Rouge1 Score': round(sum(rouge_1) / len(rouge_1), 2),\n",
    "        'Mean Rouge2 Score': round(sum(rouge_2) / len(rouge_2), 2),\n",
    "        'Mean RougeL Score': round(sum(rouge_L) / len(rouge_L), 2),\n",
    "    }\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатели метрики Rouge для модели gemma2_2b Следующие: \n",
      "{'Mean Rouge1 Score': 0.31, 'Mean Rouge2 Score': 0.16, 'Mean RougeL Score': 0.29}\n",
      "Показатели метрики Rouge для модели mistral-nemo Следующие: \n",
      "{'Mean Rouge1 Score': 0.34, 'Mean Rouge2 Score': 0.19, 'Mean RougeL Score': 0.32}\n",
      "Показатели метрики Rouge для модели gemma-2_9b Следующие: \n",
      "{'Mean Rouge1 Score': 0.36, 'Mean Rouge2 Score': 0.21, 'Mean RougeL Score': 0.34}\n"
     ]
    }
   ],
   "source": [
    "for col in needed_cols:\n",
    "    print(f\"Показатели метрики Rouge для модели {re.findall(model_extraction_pattern, col)[0]} Следующие: \")\n",
    "\n",
    "    #print(col)\n",
    "\n",
    "    print(calculate_mean_rouge_score(df = finetuned_df, df_column = f\"rouge_score_of_{col}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
