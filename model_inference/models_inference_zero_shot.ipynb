{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–Ω–Ω–∞—è —Ç–µ—Ç—Ä–∞–¥–∫–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ Zero-Soht –∏–Ω—Ñ–µ—Ä–µ–Ω—Å—É –º–æ–¥–µ–ª–µ–π Gemma2:2b, Gemma2:9b, Mistral-Nemo(12b) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Unsloth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/project_news_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—è–¥–æ–≤ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: (5197, 4)\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ gold-examples(338, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"{your_dataset_path}\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä—è–¥–æ–≤ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: {df.shape}\")\n",
    "\n",
    "def remove_garbage(txt):\n",
    "    new_line_pattern = r\"\\n\\d*\"\n",
    "    brake_pattern = r\"<br>\"\n",
    "\n",
    "    txt = re.sub(new_line_pattern, '', txt)\n",
    "    txt = re.sub(brake_pattern, '', txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "needed_cols = list(df.columns)\n",
    "\n",
    "for col in needed_cols:\n",
    "    df[col] = df[col].apply(remove_garbage)\n",
    "\n",
    "\n",
    "train_df, test_df =  train_test_split(df, test_size = 0.065, random_state = 1337)\n",
    "\n",
    "train_df = train_df.reset_index().drop(columns = ['index'])\n",
    "test_df = test_df.reset_index().drop(columns = ['index'])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ gold-examples{test_df.shape}\")\n",
    "\n",
    "train_df.to_csv('{your_dataset_path}')\n",
    "test_df.to_csv('{your_dataset_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–π –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç—Ä–∞–∂–∞–ª –±—ã —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n",
    "–ù–µ –ø–æ—è—Å–Ω—è–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç. –¢–≤–æ–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–∏—á–µ–≥–æ –±–æ–ª–µ–µ.\n",
    "\"\"\"\n",
    "\n",
    "test_df['prompt'] = model_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:\n",
    "1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.\n",
    "2. –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "3. –û—Ç–≤–µ—Ç - —Ç–≤–æ–π –æ—Ç–≤–µ—Ç.\n",
    "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.  \n",
    "### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å:\n",
    "{}\n",
    "\n",
    "### –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
    "{}\n",
    "\n",
    "### –û—Ç–≤–µ—Ç:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_headline_generation(model, tokenizer, input:str)->str:\n",
    "    '''–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –º–æ–¥–µ–ª–∏ Gemma2:2b-4bit\n",
    "    \n",
    "    Args:\n",
    "        input (str): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ - —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏\n",
    "    \n",
    "    Returns:\n",
    "        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "    '''\n",
    "    model_instructions = \"\"\"–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.\n",
    "    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–π –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç—Ä–∞–∂–∞–ª –±—ã —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n",
    "    –ù–µ –ø–æ—è—Å–Ω—è–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç. –¢–≤–æ–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–∏—á–µ–≥–æ –±–æ–ª–µ–µ.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                model_instructions,\n",
    "                input,\n",
    "                '',\n",
    "            )\n",
    "        ],return_tensors = 'pt').to('cuda')\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens = 200, use_cache = True)\n",
    "\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    answer_pattern = r\"–û—Ç–≤–µ—Ç:[^<eos>]*\"\n",
    "\n",
    "    return re.findall(answer_pattern, result[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache(model, tokenizer) -> None:\n",
    "    '''–§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ GPU'''\n",
    "    with torch.no_grad():\n",
    "        model.cpu()\n",
    "    del model\n",
    "    del tokenizer\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "models_list = ['unsloth/gemma-2-2b',\n",
    "               'unsloth/gemma-2-9b-bnb-4bit',\n",
    "               'unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit']\n",
    "\n",
    "for model in models_list:\n",
    "\n",
    "    model_name = model\n",
    "\n",
    "    print(f\"–ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model}\")\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model,\n",
    "    max_seq_length = 1048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    "    )\n",
    "    \n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(\"–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è...\")\n",
    "\n",
    "    test_df[f'{model_name}_headline'] = test_df['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏'].progress_apply(lambda x: model_headline_generation(model = model,\n",
    "                                                                                                               tokenizer = tokenizer,\n",
    "                                                                                                               input = x))\n",
    "    clear_cache(model = model,\n",
    "                tokenizer = tokenizer)\n",
    "\n",
    "test_df.to_csv('{your_dataset_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
