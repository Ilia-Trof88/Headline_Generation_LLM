{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–Ω–Ω–∞—è —Ç–µ—Ç—Ä–∞–¥—å –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å—É –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/project_news_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_examples = pd.read_csv('{your_dataset_path}')\n",
    "\n",
    "gold_examples.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "\n",
    "model_instructions = \"\"\"–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–π –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç—Ä–∞–∂–∞–ª –±—ã —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n",
    "–ù–µ –ø–æ—è—Å–Ω—è–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç. –¢–≤–æ–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–∏—á–µ–≥–æ –±–æ–ª–µ–µ.\n",
    "\"\"\"\n",
    "\n",
    "gold_examples['–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏'] = model_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:\n",
    "1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.\n",
    "2. –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "3. –û—Ç–≤–µ—Ç - —Ç–≤–æ–π –æ—Ç–≤–µ—Ç.\n",
    "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.  \n",
    "### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å:\n",
    "{}\n",
    "\n",
    "### –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
    "{}\n",
    "\n",
    "### –û—Ç–≤–µ—Ç:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = '{your_finetuned_model_path}', #–£–∫–∞–∂–∏—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –≤ –∫–æ—Ç–æ—Ä—É—é —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ LoRa-–∞–¥–∞–ø—Ç–µ—Ä—ã\n",
    "    max_seq_length = 2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(article_text: str, instructions: str) -> str:\n",
    "    '''–§—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏\n",
    "    –ø–æ —Ç–µ–∫—Å—Ç—É –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏.\n",
    "    \n",
    "    Args:\n",
    "        article_text (str): –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏.\n",
    "        instructions (str): –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è alpaca_prompt_template\n",
    "    \n",
    "    Returns:\n",
    "        str: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫'''\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                instructions,\n",
    "                article_text,\n",
    "                \"\"\n",
    "            )\n",
    "        ], return_tensors = 'pt').to('cuda')\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True, pad_token_id = tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "    result = result[0].split('–û—Ç–≤–µ—Ç:', 1)[1].replace('\\n', '').replace('</s>', '')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: \n",
      "```–ù–∞–ª–æ–≥–∏ –±–æ–ª—å—à–µ —Å–Ω–∏–∂–∞—Ç—å –Ω–µ –±—É–¥—É—Ç```\n",
      "      –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: \n",
      "```–í–ª–∞—Å—Ç–∏ –Ω–µ —Å–Ω–∏–∑—è—Ç –Ω–∞–ª–æ–≥–æ–≤—É—é –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Ç–æ–º—Å–∫–∏–π –±–∏–∑–Ω–µ—Å<eos>```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_headline = gold_examples['–ó–∞–≥–æ–ª–æ–≤–æ–∫'][50]\n",
    "\n",
    "test_article = gold_examples['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏'][50]\n",
    "\n",
    "generated_article = model_generate(article_text = test_article,\n",
    "                                   instructions = model_instructions)\n",
    "\n",
    "print(f\"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: \\n```{test_headline}```\\n\\\n",
    "      –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: \\n```{generated_article}```\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–∏–º, —á—Ç–æ –¥–∞–∂–µ –º–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å Gemma2-2b (4bit) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–ø–æ–ª–Ω–µ –æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(instructions: str, article_text: str)->str:\n",
    "    '''–§—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∑–∞—Ä–∞–Ω–µ–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é.\n",
    "    \n",
    "    Args:\n",
    "        instructions (str): –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏.\n",
    "        article_text (str): –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.\n",
    "    \n",
    "    Returns:\n",
    "        str - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.'''\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "        alpaca_prompt.format(\n",
    "            instructions,\n",
    "            article_text,\n",
    "            \"\"\n",
    "            )\n",
    "        ], return_tensors = 'pt').to('cuda')\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True, pad_token_id = tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "    result = result[0].split('–û—Ç–≤–µ—Ç:', 1)[1].replace('\\n', '').replace('</s>', '').replace('<eos>', '')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache(model, tokenizer) -> None:\n",
    "    '''–§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU'''\n",
    "    with torch.no_grad():\n",
    "        model.cpu()\n",
    "    del model\n",
    "    del tokenizer\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "models_dirs = ['unsloth/'+x for x in os.listdir('unsloth')]\n",
    "\n",
    "for model in models_dirs:\n",
    "\n",
    "    print(f'–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model}...')\n",
    "\n",
    "    model_name = model\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model,\n",
    "        max_seq_length = 2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    print(f'–ú–æ–¥–µ–ª—å {model} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞')\n",
    "\n",
    "    print(f'–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª—å—é {model}...')\n",
    "\n",
    "    gold_examples[f\"{model_name}_generated_headline\"] = gold_examples['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏'].progress_apply(lambda x: generate_headline(instructions = model_instructions,\n",
    "                                                                                                                             article_text = x))\n",
    "    \n",
    "    print('–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...')\n",
    "\n",
    "    clear_cache(model = model, tokenizer = tokenizer)\n",
    "\n",
    "    print('–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞...')\n",
    "\n",
    "gold_examples.drop(columns = ['–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏', '–¢—ç–≥–∏', '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏'], inplace = True)\n",
    "\n",
    "gold_examples.to_csv(\"{your_dataset_path}\")\n",
    "\n",
    "print('–î–∞—Ç–∞—Å–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
